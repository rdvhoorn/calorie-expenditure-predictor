{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "\n",
    "RUN_ON_KAGGLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ON_KAGGLE = True\n",
    "\n",
    "# First, upgrade scikit-learn to 1.6.1 (quietly)\n",
    "!pip install -qq scikit-learn==1.6.1\n",
    "\n",
    "# Then, reinstall category-encoders without enforcing its old scikit-learn requirement\n",
    "!pip install -qq category-encoders --no-deps --force-reinstall\n",
    "\n",
    "# Optionally, reinstall bigframes without enforcing its old rich requirement (only if you use it)\n",
    "!pip install -qq bigframes --no-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Avoid duplicate handlers\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)  # stdout works better than stderr in Jupyter\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_log_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "def rmsle_sklearn(y_true, y_pred):\n",
    "    score = rmsle(y_true, y_pred)\n",
    "    return 'rmsle', score, False\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"Calculates Root Mean Squared Logarithmic Error safely.\"\"\"\n",
    "    y_pred_clipped = np.maximum(y_pred, 0)\n",
    "    y_true_clipped = np.maximum(y_true, 0)\n",
    "    return np.sqrt(mean_squared_log_error(y_true_clipped, y_pred_clipped))\n",
    "\n",
    "def rmse_sklearn(y_true, y_pred):\n",
    "    score = rmse(y_true, y_pred)\n",
    "    return 'rmse', score, False\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Calculates Root Mean Squared Error safely.\"\"\"\n",
    "    return root_mean_squared_error(y_true, y_pred)\n",
    "\n",
    "class BaseModelWrapper:\n",
    "    def __init__(self, model_cls, model_params, eval_metric=rmsle, eval_metric_sklearn=rmsle_sklearn, name='lgbm'):\n",
    "        self.model_cls = model_cls\n",
    "        self.model_params = model_params\n",
    "        self.name = name\n",
    "        self.models = []\n",
    "        self.oof_preds = None\n",
    "        self.eval_metric = eval_metric\n",
    "        self.eval_metric_sklearn = eval_metric_sklearn\n",
    "\n",
    "    def fit(self, X, y, folds=5):\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Starting training of {self.name} model with {folds} folds\")\n",
    "        \n",
    "        self.oof_preds = np.zeros(len(X))\n",
    "        self.models = []\n",
    "        kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "            fold_start = time.time()\n",
    "            logger.info(f\"Training {self.name} - Fold {fold}/{folds}\")\n",
    "            \n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val   = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            model = self.model_cls(**self.model_params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=self.eval_metric_sklearn, \n",
    "                      callbacks=[\n",
    "                        lgb.early_stopping(stopping_rounds=100),\n",
    "                        lgb.log_evaluation(period=100)\n",
    "                    ])\n",
    "            self.oof_preds[val_idx] = model.predict(X_val)\n",
    "\n",
    "            # Calculate oof fold validation using RMSE\n",
    "            fold_rmse = self.eval_metric(y_val, self.oof_preds[val_idx])\n",
    "            logger.info(f\"Fold {fold} RMSLE: {fold_rmse:.4f}\")\n",
    "\n",
    "            self.models.append(model)\n",
    "            \n",
    "            fold_time = time.time() - fold_start\n",
    "            logger.info(f\"Completed {self.name} - Fold {fold}/{folds} in {fold_time:.2f} seconds\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate oof validation using RMSE\n",
    "        rmse = self.eval_metric(y, self.oof_preds)\n",
    "        logger.info(f\"Out-of-fold RMSLE: {rmse:.4f}\")\n",
    "\n",
    "        logger.info(f\"Completed training of {self.name} model in {total_time:.2f} seconds\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = [model.predict(X) for model in self.models]\n",
    "        return np.mean(np.column_stack(preds), axis=1)\n",
    "\n",
    "    def retrain_full(self, X, y):\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Starting full retraining of {self.name} model\")\n",
    "        \n",
    "        model = self.model_cls(**self.model_params)\n",
    "        model.fit(X, y)\n",
    "        self.models = [model]\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\"Completed full retraining of {self.name} model in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has 405 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Load data\n",
    "if RUN_ON_KAGGLE:\n",
    "    df_train = pd.read_csv('/kaggle/input/dataset/train.csv')\n",
    "    df_test = pd.read_csv('/kaggle/input/dataset/test.csv')\n",
    "else:\n",
    "    df_train = pd.read_csv('data/train.csv')\n",
    "    df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Transform target (Calories) with log1p\n",
    "df_train['Calories'] = np.log1p(df_train['Calories'])\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # Create separate columns for male and female\n",
    "    df['Sex_Male'] = (df['Sex'] == 'male').astype(int)\n",
    "    df['Sex_Female'] = (df['Sex'] == 'female').astype(int)\n",
    "    df = df.drop('Sex', axis=1)  # Drop original Sex column\n",
    "\n",
    "    # Add BMI as a feature by dividing weight by height/100 squared, normalized per gender\n",
    "    df['BMI'] = df['Weight'] / ((df['Height']/100) ** 2)\n",
    "    \n",
    "    # Normalize BMI within each gender group\n",
    "    df['BMI_Normalized'] = df.groupby(['Sex_Male', 'Sex_Female'])['BMI'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "    # Encode obesity levels based on BMI\n",
    "    df['BMI_Category'] = pd.cut(df['BMI'], \n",
    "                               bins=[0, 16.5, 18.5, 24.9, 29.9, 34.9, 39.9, float('inf')],\n",
    "                               labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "    \n",
    "\n",
    "    # Exercise intensity (heart rate / duration)\n",
    "    df['Exercise_Intensity'] = df['Heart_Rate'] / df['Duration']\n",
    "\n",
    "    # Heart rate duration\n",
    "    df['Heart_Rate_Duration'] = df['Heart_Rate'] * df['Duration']\n",
    "\n",
    "    # Temperature duration interaction\n",
    "    df['Temp_Duration'] = df['Body_Temp'] * df['Duration']\n",
    "\n",
    "    # HR divided by temp\n",
    "    df['HR_div_Temp'] = df['Heart_Rate'] / df['Body_Temp']\n",
    "\n",
    "    # Weight duration interaction\n",
    "    df['Weight_Duration'] = df['Weight'] * df['Duration']\n",
    "\n",
    "    # Max heart rate (220 - Age)\n",
    "    df['Max_Heart_Rate'] = 220 - df['Age']\n",
    "\n",
    "    # Heart rate intensity (heart rate / max heart rate)\n",
    "    df['Heart_Rate_Intensity'] = df['Heart_Rate'] / df['Max_Heart_Rate']\n",
    "\n",
    "    # Group age into bins\n",
    "    df['Age_Bins'] = pd.cut(df['Age'], bins=[0, 20, 35, 50, 100], labels=[1, 2, 3, 4])\n",
    "\n",
    "    # Get heart rate zones\n",
    "    # Zone 1\tVery Light\t50–60%\n",
    "    # Zone 2\tLight\t60–70%\n",
    "    # Zone 3\tModerate\t70–80%\n",
    "    # Zone 4\tHard\t80–90%\n",
    "    # Zone 5\tMaximum\t90-100%\n",
    "    # Heart rate zone is a percentage of max heart rate\n",
    "    df['HR_Zone'] = pd.cut(df['Heart_Rate_Intensity'] * 100,\n",
    "                          bins=[0, 50, 60, 70, 80, 90, 100],\n",
    "                          labels=[0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "\n",
    "    # Calculate BMR using Mifflin-St Jeor equation with gender-specific constant\n",
    "    df['BMR'] = (10 * df['Weight'] + \n",
    "                 6.25 * df['Height'] - \n",
    "                 5 * df['Age'] +\n",
    "                 5 * df['Sex_Male'] - \n",
    "                 161 * df['Sex_Female'])\n",
    "    \n",
    "    # Add log transformations for skewed features\n",
    "    skewed_feats = ['Age', 'Weight', 'Body_Temp', 'Height', 'Duration', 'Heart_Rate']\n",
    "    for feat in skewed_feats:\n",
    "        df[f'Log_{feat}'] = np.log1p(df[feat])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to both train and test datasets\n",
    "df_train = feature_engineering(df_train)\n",
    "df_test = feature_engineering(df_test)\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = df_train.drop(['Calories', 'id'], axis=1)\n",
    "y_train = df_train['Calories']\n",
    "\n",
    "# Standard scale X_train\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Polynomial features on X_train\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "\n",
    "# Put it back into a dataframe\n",
    "X_train = pd.DataFrame(X_train, columns=poly.get_feature_names_out())\n",
    "\n",
    "print(f\"X has {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 19:14:11,328 - INFO - Starting training of lgbm model with 3 folds\n",
      "2025-05-13 19:14:11,363 - INFO - Training lgbm - Fold 1/3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 83748\n",
      "[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 405\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 405 dense feature groups (194.55 MB) transferred to GPU in 0.182526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 4.141365\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 0.0168854\tvalid_0's rmse: 0.129944\n",
      "[200]\tvalid_0's l2: 0.00420952\tvalid_0's rmse: 0.0648808\n",
      "[300]\tvalid_0's l2: 0.00381568\tvalid_0's rmse: 0.0617712\n",
      "[400]\tvalid_0's l2: 0.00375203\tvalid_0's rmse: 0.0612538\n",
      "[500]\tvalid_0's l2: 0.00372447\tvalid_0's rmse: 0.0610284\n",
      "[600]\tvalid_0's l2: 0.0037102\tvalid_0's rmse: 0.0609114\n",
      "[700]\tvalid_0's l2: 0.00369899\tvalid_0's rmse: 0.0608193\n",
      "[800]\tvalid_0's l2: 0.00369117\tvalid_0's rmse: 0.060755\n",
      "[900]\tvalid_0's l2: 0.00368646\tvalid_0's rmse: 0.0607162\n",
      "[1000]\tvalid_0's l2: 0.00368046\tvalid_0's rmse: 0.0606668\n",
      "[1100]\tvalid_0's l2: 0.00367624\tvalid_0's rmse: 0.060632\n",
      "[1200]\tvalid_0's l2: 0.0036724\tvalid_0's rmse: 0.0606003\n",
      "[1300]\tvalid_0's l2: 0.00366968\tvalid_0's rmse: 0.0605779\n",
      "[1400]\tvalid_0's l2: 0.00366694\tvalid_0's rmse: 0.0605552\n",
      "[1500]\tvalid_0's l2: 0.00366542\tvalid_0's rmse: 0.0605427\n",
      "[1600]\tvalid_0's l2: 0.00366418\tvalid_0's rmse: 0.0605325\n",
      "[1700]\tvalid_0's l2: 0.00366257\tvalid_0's rmse: 0.0605192\n",
      "[1800]\tvalid_0's l2: 0.00366135\tvalid_0's rmse: 0.0605091\n",
      "[1900]\tvalid_0's l2: 0.00365971\tvalid_0's rmse: 0.0604955\n",
      "[2000]\tvalid_0's l2: 0.00365896\tvalid_0's rmse: 0.0604893\n",
      "Early stopping, best iteration is:\n",
      "[1958]\tvalid_0's l2: 0.00365861\tvalid_0's rmse: 0.0604865\n",
      "2025-05-13 19:16:37,044 - INFO - Fold 1 RMSLE: 0.0605\n",
      "2025-05-13 19:16:37,045 - INFO - Completed lgbm - Fold 1/3 in 145.68 seconds\n",
      "2025-05-13 19:16:37,057 - INFO - Training lgbm - Fold 2/3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 83774\n",
      "[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 405\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 405 dense feature groups (194.55 MB) transferred to GPU in 0.196402 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 4.140577\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 0.0166705\tvalid_0's rmse: 0.129114\n",
      "[200]\tvalid_0's l2: 0.00408453\tvalid_0's rmse: 0.0639103\n",
      "[300]\tvalid_0's l2: 0.00369945\tvalid_0's rmse: 0.0608231\n",
      "[400]\tvalid_0's l2: 0.00363933\tvalid_0's rmse: 0.0603269\n",
      "[500]\tvalid_0's l2: 0.00361454\tvalid_0's rmse: 0.060121\n",
      "[600]\tvalid_0's l2: 0.00359761\tvalid_0's rmse: 0.0599801\n",
      "[700]\tvalid_0's l2: 0.00358578\tvalid_0's rmse: 0.0598814\n",
      "[800]\tvalid_0's l2: 0.00357761\tvalid_0's rmse: 0.0598131\n",
      "[900]\tvalid_0's l2: 0.00356995\tvalid_0's rmse: 0.0597491\n",
      "[1000]\tvalid_0's l2: 0.00356397\tvalid_0's rmse: 0.059699\n",
      "[1100]\tvalid_0's l2: 0.00356017\tvalid_0's rmse: 0.0596672\n",
      "[1200]\tvalid_0's l2: 0.00355729\tvalid_0's rmse: 0.059643\n",
      "[1300]\tvalid_0's l2: 0.0035538\tvalid_0's rmse: 0.0596138\n",
      "[1400]\tvalid_0's l2: 0.00355147\tvalid_0's rmse: 0.0595942\n",
      "[1500]\tvalid_0's l2: 0.00355004\tvalid_0's rmse: 0.0595822\n",
      "[1600]\tvalid_0's l2: 0.00354837\tvalid_0's rmse: 0.0595682\n",
      "[1700]\tvalid_0's l2: 0.00354709\tvalid_0's rmse: 0.0595575\n",
      "[1800]\tvalid_0's l2: 0.00354707\tvalid_0's rmse: 0.0595573\n",
      "Early stopping, best iteration is:\n",
      "[1707]\tvalid_0's l2: 0.00354681\tvalid_0's rmse: 0.0595551\n",
      "2025-05-13 19:18:53,108 - INFO - Fold 2 RMSLE: 0.0596\n",
      "2025-05-13 19:18:53,109 - INFO - Completed lgbm - Fold 2/3 in 136.05 seconds\n",
      "2025-05-13 19:18:53,123 - INFO - Training lgbm - Fold 3/3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 83783\n",
      "[LightGBM] [Info] Number of data points in the train set: 500000, number of used features: 405\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 405 dense feature groups (194.55 MB) transferred to GPU in 0.181756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 4.141492\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l2: 0.0165989\tvalid_0's rmse: 0.128837\n",
      "[200]\tvalid_0's l2: 0.00412803\tvalid_0's rmse: 0.0642498\n",
      "[300]\tvalid_0's l2: 0.00376187\tvalid_0's rmse: 0.0613341\n",
      "[400]\tvalid_0's l2: 0.00370571\tvalid_0's rmse: 0.0608745\n",
      "[500]\tvalid_0's l2: 0.00368056\tvalid_0's rmse: 0.0606676\n",
      "[600]\tvalid_0's l2: 0.0036655\tvalid_0's rmse: 0.0605433\n",
      "[700]\tvalid_0's l2: 0.00365344\tvalid_0's rmse: 0.0604437\n",
      "[800]\tvalid_0's l2: 0.00364538\tvalid_0's rmse: 0.060377\n",
      "[900]\tvalid_0's l2: 0.00363882\tvalid_0's rmse: 0.0603226\n",
      "[1000]\tvalid_0's l2: 0.00363211\tvalid_0's rmse: 0.060267\n",
      "[1100]\tvalid_0's l2: 0.00362926\tvalid_0's rmse: 0.0602433\n",
      "[1200]\tvalid_0's l2: 0.00362664\tvalid_0's rmse: 0.0602216\n",
      "[1300]\tvalid_0's l2: 0.00362405\tvalid_0's rmse: 0.0602001\n",
      "[1400]\tvalid_0's l2: 0.0036216\tvalid_0's rmse: 0.0601798\n",
      "[1500]\tvalid_0's l2: 0.00361959\tvalid_0's rmse: 0.060163\n",
      "[1600]\tvalid_0's l2: 0.00361842\tvalid_0's rmse: 0.0601533\n",
      "[1700]\tvalid_0's l2: 0.00361678\tvalid_0's rmse: 0.0601396\n",
      "[1800]\tvalid_0's l2: 0.00361537\tvalid_0's rmse: 0.0601279\n",
      "[1900]\tvalid_0's l2: 0.00361502\tvalid_0's rmse: 0.0601251\n",
      "[2000]\tvalid_0's l2: 0.00361389\tvalid_0's rmse: 0.0601156\n",
      "[2100]\tvalid_0's l2: 0.00361334\tvalid_0's rmse: 0.060111\n",
      "[2200]\tvalid_0's l2: 0.00361306\tvalid_0's rmse: 0.0601087\n",
      "Early stopping, best iteration is:\n",
      "[2136]\tvalid_0's l2: 0.00361287\tvalid_0's rmse: 0.0601071\n",
      "2025-05-13 19:21:27,888 - INFO - Fold 3 RMSLE: 0.0601\n",
      "2025-05-13 19:21:27,889 - INFO - Completed lgbm - Fold 3/3 in 154.77 seconds\n",
      "2025-05-13 19:21:27,896 - INFO - Out-of-fold RMSLE: 0.0601\n",
      "2025-05-13 19:21:27,896 - INFO - Completed training of lgbm model in 436.56 seconds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm_model = BaseModelWrapper(\n",
    "    lgb.LGBMRegressor,\n",
    "    {\n",
    "        'objective': 'regression',\n",
    "        'seed': 42,\n",
    "        'n_estimators': 3000,\n",
    "        'learning_rate': 0.022,\n",
    "        'max_depth': -1,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'min_child_weight': 1,\n",
    "        'verbose': 1,\n",
    "        'device': 'gpu'\n",
    "    },\n",
    "    eval_metric=rmse,\n",
    "    eval_metric_sklearn=rmse_sklearn,\n",
    "    name='lgbm'\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train, y_train, folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'objective': 'regression',\n",
    "    'seed': 42,\n",
    "    'n_estimators': 3000,\n",
    "    'learning_rate': 0.022,\n",
    "    'max_depth': -1,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'min_child_weight': 1,\n",
    "    'verbose': 1,\n",
    "    'device': 'gpu'\n",
    "},\n",
    "\n",
    "run 1: 0.0604865\n",
    "run 2: 0.0595551\n",
    "run 3: 0.0601071\n",
    "\n",
    "oof 0.0601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale oof preds with expm1 and save to npy file\n",
    "scaled_preds = np.expm1(lgbm_model.oof_preds)\n",
    "\n",
    "if RUN_ON_KAGGLE:\n",
    "    np.save('lgbm2_oof_preds.npy', scaled_preds)\n",
    "else:\n",
    "    np.save('data/ensemble/lgbm2_oof_preds.npy', scaled_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature names and importances\n",
    "feature_names = X_train.columns\n",
    "importances = lgbm2_model.models[0].feature_importances_\n",
    "\n",
    "n_cols = 22\n",
    "\n",
    "# Sort features by importance and take top 20\n",
    "indices = np.argsort(importances)[::-1][:n_cols]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(f'Top {n_cols} Feature Importances')\n",
    "plt.bar(range(n_cols), importances[indices])\n",
    "plt.xticks(range(n_cols), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[feature_names[i] for i in np.argsort(importances)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'id' column before dropping it\n",
    "test_ids = df_test['id']\n",
    "\n",
    "# Drop 'id' column before prediction\n",
    "df_test_without_id = df_test.drop('id', axis=1)\n",
    "\n",
    "X_test = scaler.transform(df_test_without_id)\n",
    "\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "# Put it back into a dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=poly.get_feature_names_out())\n",
    "\n",
    "# Make predictions on test data\n",
    "test_preds = lgbm_model.predict(X_test)\n",
    "\n",
    "# Scale test preds with expm1\n",
    "scaled_test_preds = np.expm1(test_preds)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Calories': scaled_test_preds\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "if RUN_ON_KAGGLE:\n",
    "    submission.to_csv('lgbm2_submission.csv', index=False)\n",
    "else:\n",
    "    submission.to_csv('data/ensemble/lgbm2_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
