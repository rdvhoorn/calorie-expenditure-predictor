{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Avoid duplicate handlers\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler(sys.stdout)  # stdout works better than stderr in Jupyter\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "class BaseModelWrapper:\n",
    "    def __init__(self, model_cls, model_params, name, preprocessor=None):\n",
    "        self.model_cls = model_cls\n",
    "        self.model_params = model_params\n",
    "        self.name = name\n",
    "        self.preprocessor = preprocessor\n",
    "        self.models = []\n",
    "        self.oof_preds = None\n",
    "\n",
    "    def _prep(self, X, fit=False):\n",
    "        return self.preprocessor(X, fit=fit) if self.preprocessor else X\n",
    "\n",
    "    def fit(self, X, y, folds=5):\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Starting training of {self.name} model with {folds} folds\")\n",
    "        \n",
    "        self.oof_preds = np.zeros(len(X))\n",
    "        self.models = []\n",
    "        kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "            fold_start = time.time()\n",
    "            logger.info(f\"Training {self.name} - Fold {fold}/{folds}\")\n",
    "            \n",
    "            X_train = self._prep(X.iloc[train_idx], fit=True)\n",
    "            X_val   = self._prep(X.iloc[val_idx], fit=False)\n",
    "            y_train = y.iloc[train_idx]\n",
    "\n",
    "            model = self.model_cls(**self.model_params)\n",
    "            model.fit(X_train, y_train)\n",
    "            self.oof_preds[val_idx] = model.predict(X_val)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            fold_time = time.time() - fold_start\n",
    "            logger.info(f\"Completed {self.name} - Fold {fold}/{folds} in {fold_time:.2f} seconds\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\"Completed training of {self.name} model in {total_time:.2f} seconds\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_proc = self._prep(X, fit=False)\n",
    "        preds = [model.predict(X_proc) for model in self.models]\n",
    "        return np.mean(np.column_stack(preds), axis=1)\n",
    "\n",
    "    def retrain_full(self, X, y):\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Starting full retraining of {self.name} model\")\n",
    "        \n",
    "        X_proc = self._prep(X, fit=True)\n",
    "        model = self.model_cls(**self.model_params)\n",
    "        model.fit(X_proc, y)\n",
    "        self.models = [model]\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\"Completed full retraining of {self.name} model in {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "class StackingEnsembler:\n",
    "    def __init__(self, base_models, meta_model_cls, meta_model_params, meta_preprocessor=None):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model_cls = meta_model_cls\n",
    "        self.meta_model_params = meta_model_params\n",
    "        self.meta_preprocessor = meta_preprocessor\n",
    "        self.meta_model = None\n",
    "\n",
    "    def _prep(self, X, fit=False):\n",
    "        return self.meta_preprocessor(X, fit=fit) if self.meta_preprocessor else X\n",
    "    \n",
    "    def fit(self, X, y, folds=5):\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Starting stacking ensemble training with {len(self.base_models)} base models\")\n",
    "        \n",
    "        # Train base models and collect OOF predictions\n",
    "        oof_features = []\n",
    "        for i, model in enumerate(self.base_models, 1):\n",
    "            logger.info(f\"Training base model {i}/{len(self.base_models)}: {model.name}\")\n",
    "            model.fit(X, y, folds=folds)\n",
    "            oof_features.append(model.oof_preds.reshape(-1, 1))\n",
    "\n",
    "        logger.info(\"Training meta-model\")\n",
    "        meta_start = time.time()\n",
    "        \n",
    "        meta_X = np.hstack(oof_features)\n",
    "        meta_X = self._prep(meta_X, fit=True)\n",
    "\n",
    "        self.meta_model = self.meta_model_cls(**self.meta_model_params)\n",
    "        self.meta_model.fit(meta_X, y)\n",
    "        \n",
    "        meta_time = time.time() - meta_start\n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\"Meta-model training completed in {meta_time:.2f} seconds\")\n",
    "        logger.info(f\"Total stacking ensemble training completed in {total_time:.2f} seconds\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        logger.info(\"Generating predictions from stacking ensemble\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        base_preds = [model.predict(X).reshape(-1, 1) for model in self.base_models]\n",
    "        meta_X = np.hstack(base_preds)\n",
    "        meta_X = self._prep(meta_X, fit=False)\n",
    "        predictions = self.meta_model.predict(meta_X)\n",
    "        \n",
    "        pred_time = time.time() - start_time\n",
    "        logger.info(f\"Predictions generated in {pred_time:.2f} seconds\")\n",
    "        return predictions \n",
    "    \n",
    "    def retrain_base_models(self, X, y):\n",
    "        logger.info(\"Retraining base models on full dataset\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, base_model in enumerate(self.base_models, 1):\n",
    "            logger.info(f\"Retraining base model {i}/{len(self.base_models)}: {base_model.name}\")\n",
    "            base_model.retrain_full(X, y)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        logger.info(f\"Base model retraining completed in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class StandardScalerPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "\n",
    "    def __call__(self, X, fit=False):\n",
    "        if fit:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            self.fitted = True\n",
    "        else:\n",
    "            if not self.fitted:\n",
    "                raise RuntimeError(\"Preprocessor not fitted\")\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = BaseModelWrapper(\n",
    "    lgb.LGBMRegressor,\n",
    "    {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': 1024,\n",
    "        'max_bin': 1024,\n",
    "        'learning_rate': 0.02,\n",
    "        'subsample': 0.8,\n",
    "        'n_estimators': 1000,\n",
    "        'verbose': 1\n",
    "    },\n",
    "    name='lgbm'\n",
    ")\n",
    "\n",
    "lasso_model = BaseModelWrapper(\n",
    "    Lasso,\n",
    "    {'alpha': 0.005, 'max_iter': 10000},\n",
    "    preprocessor=StandardScalerPreprocessor(),\n",
    "    name='lasso'\n",
    ")\n",
    "\n",
    "# Now use them in wrappers\n",
    "base_models = [\n",
    "    lgbm_model,\n",
    "    lasso_model,\n",
    "]\n",
    "\n",
    "stack = StackingEnsembler(\n",
    "    base_models=base_models,\n",
    "    meta_model_cls=Ridge,\n",
    "    meta_model_params={'alpha': 0.1},\n",
    "    meta_preprocessor=StandardScalerPreprocessor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Transform Sex to 0 and 1\n",
    "df_train['Sex'] = df_train['Sex'].map({'male': 0, 'female': 1})\n",
    "df_test['Sex'] = df_test['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Transform target (Calories) with log1p\n",
    "df_train['Calories'] = np.log1p(df_train['Calories'])\n",
    "\n",
    "# Create column interactions between all numerical columns\n",
    "numeric_cols = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
    "\n",
    "# Create interaction features\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "        interaction_name = f'{col1}_{col2}_interaction'\n",
    "        df_train[interaction_name] = df_train[col1] * df_train[col2]\n",
    "        df_test[interaction_name] = df_test[col1] * df_test[col2]\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = df_train.drop(['Calories', 'id'], axis=1)\n",
    "y_train = df_train['Calories']\n",
    "\n",
    "# Split data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:42:43,178 - INFO - Starting stacking ensemble training with 2 base models\n",
      "2025-05-07 08:42:43,192 - INFO - Training base model 1/2: lgbm\n",
      "2025-05-07 08:42:43,193 - INFO - Starting training of lgbm model with 5 folds\n",
      "2025-05-07 08:42:43,228 - INFO - Training lgbm - Fold 1/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14659\n",
      "[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 4.140083\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train level 2 model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstack\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Retrain base models on full data\u001b[39;00m\n\u001b[32m      5\u001b[39m stack.retrain_base_models(X_train, y_train)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mStackingEnsembler.fit\u001b[39m\u001b[34m(self, X, y, folds)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_models, \u001b[32m1\u001b[39m):\n\u001b[32m     82\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining base model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     oof_features.append(model.oof_preds.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m     86\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTraining meta-model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mBaseModelWrapper.fit\u001b[39m\u001b[34m(self, X, y, folds)\u001b[39m\n\u001b[32m     33\u001b[39m y_train = y.iloc[train_idx]\n\u001b[32m     35\u001b[39m model = \u001b[38;5;28mself\u001b[39m.model_cls(**\u001b[38;5;28mself\u001b[39m.model_params)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.oof_preds[val_idx] = model.predict(X_val)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.models.append(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rvhoo\\Documents\\projects\\calorie-expenditure-predictor\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[39m, in \u001b[36mLGBMRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1383\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1395\u001b[39m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1396\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLGBMRegressor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rvhoo\\Documents\\projects\\calorie-expenditure-predictor\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rvhoo\\Documents\\projects\\calorie-expenditure-predictor\\.venv\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rvhoo\\Documents\\projects\\calorie-expenditure-predictor\\.venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train level 2 model\n",
    "stack.fit(X_train, y_train, folds=5)\n",
    "\n",
    "# Retrain base models on full data\n",
    "stack.retrain_base_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_val_pred = stack.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(y_val, y_val_pred)\n",
    "print(f\"RMSE on validation set: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "\n",
    "# Save ids from test set\n",
    "test_ids = df_test['id']\n",
    "\n",
    "# Remove id column from test set\n",
    "df_test = df_test.drop('id', axis=1)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = stack.predict(df_test)\n",
    "\n",
    "# Inverse transform predictions using expm1\n",
    "y_test_pred = np.expm1(y_test_pred)\n",
    "\n",
    "# Create submission file with ids and predictions\n",
    "submission = pd.DataFrame({'id': test_ids, 'Calories': y_test_pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
